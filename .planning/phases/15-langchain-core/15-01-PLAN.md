---
phase: 15-langchain-core
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - plugins/langchain-agent/requirements.txt
  - plugins/langchain-agent/plugin.py
autonomous: true
user_setup:
  - service: openai
    why: "LLM provider for OpenAI bot"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard -> API keys -> Create new secret key"
    dashboard_config: []
  - service: anthropic
    why: "LLM provider for Anthropic bot"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys -> Create Key"
    dashboard_config: []

must_haves:
  truths:
    - "User messages to OpenAI bot receive AI-generated responses"
    - "User messages to Anthropic bot receive AI-generated responses"
    - "Bots gracefully handle missing API keys"
    - "Bots gracefully handle API errors"
  artifacts:
    - path: "plugins/langchain-agent/requirements.txt"
      provides: "LangChain dependencies"
      contains: "langchain"
    - path: "plugins/langchain-agent/plugin.py"
      provides: "LangChain model initialization and invocation"
      contains: "ChatOpenAI"
  key_links:
    - from: "plugins/langchain-agent/plugin.py"
      to: "langchain_openai.ChatOpenAI"
      via: "import and instantiation in on_activate"
      pattern: "self\\.openai_model.*=.*ChatOpenAI"
    - from: "plugins/langchain-agent/plugin.py"
      to: "model.invoke(messages)"
      via: "LLM invocation in handlers"
      pattern: "self\\.openai_model\\.invoke"
---

<objective>
Replace placeholder bot handlers with real LangChain LLM integration.

Purpose: Enable the OpenAI and Anthropic bots to generate actual AI responses using LangChain's unified interface, demonstrating Python ecosystem advantages.

Output: Working AI chat bots that respond using GPT-4o (OpenAI) and Claude Sonnet (Anthropic).
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-langchain-core/15-RESEARCH.md
@.planning/phases/14-bot-infrastructure/14-01-SUMMARY.md
@plugins/langchain-agent/plugin.py
@plugins/langchain-agent/requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add LangChain dependencies and model initialization</name>
  <files>plugins/langchain-agent/requirements.txt, plugins/langchain-agent/plugin.py</files>
  <action>
1. Update requirements.txt to include LangChain packages:
   ```
   mattermost-plugin
   langchain>=1.2.0
   langchain-openai>=1.1.0
   langchain-anthropic>=1.3.0
   ```

2. Add imports at top of plugin.py (after existing imports):
   ```python
   from langchain_openai import ChatOpenAI
   from langchain_anthropic import ChatAnthropic
   from langchain_core.messages import HumanMessage, SystemMessage
   ```

3. Add model instance variables in `__init__`:
   ```python
   self.openai_model: ChatOpenAI | None = None
   self.anthropic_model: ChatAnthropic | None = None
   ```

4. Initialize models in `on_activate` (AFTER bot creation, before final log):
   ```python
   # Initialize LangChain models
   try:
       self.openai_model = ChatOpenAI(model="gpt-4o", temperature=0.7)
       self.logger.info("OpenAI model initialized")
   except Exception as e:
       self.logger.warning(f"OpenAI model not available: {e}")
       self.openai_model = None

   try:
       self.anthropic_model = ChatAnthropic(model="claude-sonnet-4-5-20250929", temperature=0.7)
       self.logger.info("Anthropic model initialized")
   except Exception as e:
       self.logger.warning(f"Anthropic model not available: {e}")
       self.anthropic_model = None
   ```

IMPORTANT: Models read API keys from environment variables automatically (OPENAI_API_KEY, ANTHROPIC_API_KEY). Do NOT hardcode keys.
  </action>
  <verify>
- `grep -q "langchain" plugins/langchain-agent/requirements.txt` succeeds
- `grep -q "ChatOpenAI" plugins/langchain-agent/plugin.py` succeeds
- `grep -q "ChatAnthropic" plugins/langchain-agent/plugin.py` succeeds
- `grep -q "openai_model" plugins/langchain-agent/plugin.py` succeeds
  </verify>
  <done>
- requirements.txt has langchain, langchain-openai, langchain-anthropic dependencies
- plugin.py imports ChatOpenAI, ChatAnthropic, and message types
- plugin.py initializes models in on_activate with error handling
  </done>
</task>

<task type="auto">
  <name>Task 2: Replace placeholder handlers with LangChain invocation</name>
  <files>plugins/langchain-agent/plugin.py</files>
  <action>
Replace both `_handle_openai_message` and `_handle_anthropic_message` methods with real LangChain invocations:

1. Replace `_handle_openai_message`:
   ```python
   def _handle_openai_message(self, post: Post) -> None:
       """
       Handle a message directed to the OpenAI bot using LangChain.

       Args:
           post: The Post object containing the user's message.
       """
       self.logger.info(f"OpenAI Agent processing message: {post.message[:50]}...")

       if self.openai_model is None:
           self._send_error_response(post.channel_id, "OpenAI not configured. Check OPENAI_API_KEY.")
           return

       messages = [
           SystemMessage(content="You are a helpful AI assistant powered by OpenAI. Be concise and helpful."),
           HumanMessage(content=post.message),
       ]

       try:
           response = self.openai_model.invoke(messages)
           self._send_response(post.channel_id, response.content)
           self.logger.debug("OpenAI Agent sent response")
       except Exception as e:
           self.logger.error(f"OpenAI API error: {e}")
           self._send_error_response(post.channel_id, f"OpenAI error: {e}")
   ```

2. Replace `_handle_anthropic_message`:
   ```python
   def _handle_anthropic_message(self, post: Post) -> None:
       """
       Handle a message directed to the Anthropic bot using LangChain.

       Args:
           post: The Post object containing the user's message.
       """
       self.logger.info(f"Anthropic Agent processing message: {post.message[:50]}...")

       if self.anthropic_model is None:
           self._send_error_response(post.channel_id, "Anthropic not configured. Check ANTHROPIC_API_KEY.")
           return

       messages = [
           SystemMessage(content="You are a helpful AI assistant powered by Anthropic Claude. Be concise and helpful."),
           HumanMessage(content=post.message),
       ]

       try:
           response = self.anthropic_model.invoke(messages)
           self._send_response(post.channel_id, response.content)
           self.logger.debug("Anthropic Agent sent response")
       except Exception as e:
           self.logger.error(f"Anthropic API error: {e}")
           self._send_error_response(post.channel_id, f"Anthropic error: {e}")
   ```

3. Add helper methods for response sending:
   ```python
   def _send_response(self, channel_id: str, message: str) -> None:
       """Send a response message to the channel."""
       try:
           response = Post(id="", channel_id=channel_id, message=message)
           self.api.create_post(response)
       except Exception as e:
           self.logger.error(f"Failed to send response: {e}")

   def _send_error_response(self, channel_id: str, error: str) -> None:
       """Send an error message to the channel."""
       self._send_response(channel_id, f"Sorry, I encountered an error: {error}")
   ```

IMPORTANT: 
- Keep SystemMessage content brief and focused
- Use response.content to extract text from AIMessage
- Log errors but also inform user via _send_error_response
  </action>
  <verify>
- `grep -q "invoke" plugins/langchain-agent/plugin.py` succeeds
- `grep -q "_send_response" plugins/langchain-agent/plugin.py` succeeds
- `grep -q "_send_error_response" plugins/langchain-agent/plugin.py` succeeds
- No placeholder "[OpenAI Agent] Received:" text remains: `grep -c "Received:" plugins/langchain-agent/plugin.py` returns 0
  </verify>
  <done>
- _handle_openai_message uses self.openai_model.invoke() with proper messages
- _handle_anthropic_message uses self.anthropic_model.invoke() with proper messages
- Both handlers check for None model and return helpful error
- Both handlers catch exceptions and inform user
- Helper methods _send_response and _send_error_response exist
- No placeholder response text remains
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. **Static checks:**
   - `grep -E "langchain|ChatOpenAI|ChatAnthropic" plugins/langchain-agent/plugin.py` shows all required imports and usage
   - `grep "invoke" plugins/langchain-agent/plugin.py` shows model invocations

2. **Syntax check:**
   - `python3 -m py_compile plugins/langchain-agent/plugin.py` succeeds (no syntax errors)

3. **Manual verification (requires API keys):**
   - DM the OpenAI bot with "What is 2+2?" - should get AI response
   - DM the Anthropic bot with "What is 2+2?" - should get AI response
</verification>

<success_criteria>
- LangChain dependencies added to requirements.txt
- ChatOpenAI and ChatAnthropic models initialized on plugin activation
- Placeholder handlers replaced with real LLM invocations
- Error handling for missing API keys and API failures
- Plugin compiles without syntax errors
</success_criteria>

<output>
After completion, create `.planning/phases/15-langchain-core/15-01-SUMMARY.md`
</output>
