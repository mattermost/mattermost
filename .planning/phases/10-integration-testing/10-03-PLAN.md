---
phase: 10-integration-testing
plan: 03
type: execute
depends_on: ["10-02"]
files_modified:
  - server/public/pluginapi/grpc/server/benchmark_test.go
  - python-sdk/tests/benchmark_test.py
  - docs/python-plugins.md
---

<objective>
Create performance benchmarks and comprehensive documentation for Python plugin development.

Purpose: Establish baseline performance metrics comparing Python plugins to Go plugins, and provide clear documentation for plugin developers.
Output: Benchmark suites and developer documentation.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# SDK public interface
@python-sdk/src/mattermost_plugin/__init__.py

# Example plugin for documentation reference
# (Will be created by 10-01)

**Tech stack available:** Go testing.B benchmarks, pytest-benchmark (optional)
**Established patterns:** Go benchmark tests with `testing.B`, Python SDK docstrings
**Constraining decisions:**
- PROJECT.md: "Performance parity guarantees - Python will have inherent overhead vs Go" (out of scope to match, but should measure)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Go benchmark tests for gRPC overhead</name>
  <files>server/public/pluginapi/grpc/server/benchmark_test.go</files>
  <action>
Create Go benchmark tests measuring gRPC server performance.

Benchmarks to implement:
1. **BenchmarkAPIGetUser**: Measure GetUser RPC latency
2. **BenchmarkAPICreatePost**: Measure CreatePost RPC latency
3. **BenchmarkHookInvocation**: Measure hook round-trip (call hook, receive response)
4. **BenchmarkServeHTTP**: Measure HTTP streaming overhead per request

Implementation:
```go
func BenchmarkAPIGetUser(b *testing.B) {
    // Setup mock API and gRPC server
    // b.ResetTimer()
    // for i := 0; i < b.N; i++ {
    //     client.GetUser(ctx, &pb.GetUserRequest{UserId: "test"})
    // }
}
```

Use mock plugin.API implementation to isolate gRPC overhead from actual database calls.

Report:
- Operations per second
- Latency percentiles (via b.ReportMetric if available)
- Memory allocations (b.ReportAllocs())

These benchmarks establish baseline for comparing Python plugin overhead vs native Go RPC.
  </action>
  <verify>cd server/public/pluginapi/grpc/server && go test -bench=. -benchmem -count=1 -run=^$ 2>&1 | head -20</verify>
  <done>Benchmark tests run and report ops/sec and memory allocations</done>
</task>

<task type="auto">
  <name>Task 2: Create Python benchmark tests</name>
  <files>python-sdk/tests/benchmark_test.py</files>
  <action>
Create Python benchmark tests measuring SDK performance.

Benchmarks to implement (using pytest-benchmark or simple timing):
1. **test_benchmark_api_get_user**: Measure API call latency
2. **test_benchmark_wrapper_conversion**: Measure proto-to-wrapper overhead
3. **test_benchmark_hook_handler_dispatch**: Measure hook decorator dispatch overhead

Implementation approach:
- If pytest-benchmark available, use `benchmark` fixture
- Otherwise, use simple timing with `time.perf_counter()` and report averages

```python
def test_benchmark_api_get_user(benchmark):
    """Benchmark GetUser API call latency."""
    client = create_mock_client()
    result = benchmark(client.get_user, "test-user-id")
    assert result is not None
```

Add to requirements (dev extras) if pytest-benchmark not present.

Focus on measuring:
- gRPC call overhead
- Protobuf serialization/deserialization
- Wrapper class instantiation
  </action>
  <verify>cd python-sdk && python -m pytest tests/benchmark_test.py -v 2>&1 | head -20</verify>
  <done>Python benchmarks run and report timing results</done>
</task>

<task type="auto">
  <name>Task 3: Create Python plugin developer documentation</name>
  <files>docs/python-plugins.md</files>
  <action>
Create comprehensive documentation for Python plugin developers.

Sections to include:

1. **Introduction**
   - What Python plugins enable
   - When to use Python vs Go plugins
   - Performance considerations

2. **Getting Started**
   - Prerequisites (Python 3.9+, Mattermost server version)
   - Creating a plugin project
   - Plugin structure (plugin.json, plugin.py, requirements.txt)

3. **Plugin Manifest**
   - `server.runtime: "python"` field
   - `server.python.entry_point` field
   - `server.python_version` field (informational)

4. **SDK Reference**
   - Plugin base class
   - Hook decorator (`@hook(HookName.X)`)
   - API client access (`self.api`)
   - Logger access (`self.logger`)

5. **Hook Reference**
   - Hook categories (lifecycle, message, user, channel, etc.)
   - Return value semantics (allow/reject/modify patterns)
   - Example for each major hook type

6. **API Reference**
   - Overview of available API methods
   - Error handling (PluginAPIError and subclasses)
   - Async vs sync clients

7. **ServeHTTP**
   - Handling HTTP requests
   - Request/response streaming
   - Example HTTP handler

8. **Best Practices**
   - Error handling patterns
   - Logging guidelines
   - Testing your plugin

Keep documentation concise but complete. Reference example plugin for concrete usage.
  </action>
  <verify>test -f docs/python-plugins.md && wc -l docs/python-plugins.md</verify>
  <done>Documentation file exists with all required sections</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Go benchmarks run: `go test -bench=. ./server/public/pluginapi/grpc/server/`
- [ ] Python benchmarks run (may use simple timing if pytest-benchmark not available)
- [ ] Documentation covers all major topics
- [ ] Documentation references example plugin
</verification>

<success_criteria>

- All tasks completed
- Performance benchmarks establish baseline metrics
- Documentation provides clear path for Python plugin development
- Phase 10 complete, project milestone achieved
</success_criteria>

<output>
After completion, create `.planning/phases/10-integration-testing/10-03-SUMMARY.md`:

# Phase 10 Plan 03: Performance Benchmarks and Documentation Summary

**[Substantive one-liner - what shipped]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `server/public/pluginapi/grpc/server/benchmark_test.go` - Go benchmarks
- `python-sdk/tests/benchmark_test.py` - Python benchmarks
- `docs/python-plugins.md` - Developer documentation

## Decisions Made

[Key decisions and rationale, or "None"]

## Performance Results

[Summary of benchmark results if available]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Phase 10 complete. All planned phases implemented. Project ready for review and next milestone planning.
</output>
